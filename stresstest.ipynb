{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "from src.factryengine.models import Resource, Task, Assignment, ResourceGroup\n",
    "from src.factryengine.scheduler.core import Scheduler\n",
    "from src.factryengine.scheduler.task_batch_processor import TaskSplitter\n",
    "\n",
    "\n",
    "class ProdScheduler: \n",
    "    def __init__(self, \n",
    "                resource_dir: str, \n",
    "                resource_group_dir: str, \n",
    "                task_dir: str\n",
    "                 ) -> None:\n",
    "        # Scheduler Attributes\n",
    "        self.cph_timezone = pytz.timezone('Europe/Copenhagen')\n",
    "        # self.today = datetime.now(timezone.utc).replace(\n",
    "        #     hour=0, minute=0, second=0, microsecond=0)\n",
    "        self.today = datetime(2024, 10, 9, 0, 0, 0, 0, tzinfo=timezone.utc)\n",
    "        self.today_str = str(self.today)[:19]\n",
    "\n",
    "        # Component Attributes\n",
    "        self.dict_resource = {}\n",
    "        self.dict_resourcegroups = {}\n",
    "        self.tasks_list = []\n",
    "        self.task_dict = {}\n",
    "        self.pred_dict = {}\n",
    "        self.flow_map = {}\n",
    "        self.pred_exploded = {}\n",
    "\n",
    "        # Inputs \n",
    "        with open(resource_dir, 'r') as file:\n",
    "            self.r_data = json.load(file)\n",
    "\n",
    "        with open(task_dir, 'r') as file:\n",
    "            self.t_data = json.load(file)\n",
    "\n",
    "        with open(resource_group_dir, 'r') as file:\n",
    "            self.rg_data = json.load(file)\n",
    "    \n",
    "    def convert_to_minutes(self, datetime_str, start_time_obj):\n",
    "        datetime_obj = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "        diff_minutes = (datetime_obj - start_time_obj).total_seconds()/60\n",
    "        return int(diff_minutes)\n",
    "\n",
    "    def adjust_capacity(self, start, end, capacity):\n",
    "        return (end - start) * capacity + start\n",
    "\n",
    "    def organize_predecessors(self, task: Task):\n",
    "        try:\n",
    "            list_predecessors = self.pred_dict[task.id]\n",
    "            # ============================================================== UNCOMMENT TO USE MICROBATCH FLOW\n",
    "            if task.batch_id:  # Check if task is microbatched\n",
    "                # Look for each predecessors that exist in the flow map\n",
    "                for predecessor in list_predecessors:\n",
    "                    pred_batch_id = f'{predecessor}-{task.batch_id}'\n",
    "                    if pred_batch_id in self.flow_map and pred_batch_id in self.pred_dict:  # Check if pred is part of flow\n",
    "                        # Check if pred-parent connection is correct\n",
    "                        if self.flow_map[task.id]['predecessor'] == self.flow_map[pred_batch_id]['parent']:\n",
    "                            self.pred_dict[task.id] = [pred_batch_id]\n",
    "\n",
    "                    elif pred_batch_id not in self.pred_dict and predecessor not in self.task_dict:\n",
    "                        parent_predecessor = []\n",
    "                        for pred in self.pred_dict[task.id]:\n",
    "                            parent_predecessor.extend(self.pred_dict[pred])\n",
    "                            self.pred_dict[task.id] = parent_predecessor\n",
    "            # ============================================================== UNCOMMENT TO USE MICROBATCH FLOW\n",
    "\n",
    "            # Remove task batch id\n",
    "            task.set_batch_id(None)\n",
    "\n",
    "            # Check for predecessors to be exploded\n",
    "            for predecessor in list_predecessors:\n",
    "                if predecessor in self.pred_exploded:\n",
    "                    self.pred_dict[task.id].remove(\n",
    "                        predecessor)  # Remove original value\n",
    "                    self.pred_dict[task.id].extend(\n",
    "                        self.pred_exploded[predecessor])  # Add exploded batches\n",
    "        except Exception as e:\n",
    "            return\n",
    "    \n",
    "    def set_predecessors(self, task: Task):\n",
    "        if not task.id in self.pred_dict:  # if task is not in pred_dict, then it has no predecessors\n",
    "            return\n",
    "\n",
    "        for pred_id in self.pred_dict[task.id]:\n",
    "            if pred_id in self.task_dict:  # ensure predecessor exists in task_dict\n",
    "                pred_task = self.task_dict[pred_id]\n",
    "\n",
    "                # Avoid adding a predecessor multiple times\n",
    "                if pred_task not in task.predecessors:\n",
    "                    # set predecessors for the predecessor first\n",
    "                    self.set_predecessors(pred_task)\n",
    "                    task.predecessors.append(pred_task)\n",
    "    \n",
    "    def create_resource_object(self, resource_list):\n",
    "        # Generate slot based on schedule selected in NocoDB\n",
    "        for row in resource_list:\n",
    "            periods_list = []\n",
    "            for sched in row['availability']:\n",
    "                if not sched['is_absent']:\n",
    "                    start = self.convert_to_minutes(\n",
    "                        sched['start_datetime'], self.today)\n",
    "                    end = self.convert_to_minutes(\n",
    "                        sched['end_datetime'], self.today)\n",
    "                    # ========= Uncomment to use capacity\n",
    "                    capacity = sched['capacity_percent']\n",
    "                    # capacity = None\n",
    "                    periods_list.append((int(start), int(self.adjust_capacity(\n",
    "                        start, end, capacity)) if capacity else int(end)))\n",
    "\n",
    "            resource_id = int(row['resource_id'])\n",
    "            self.dict_resource[resource_id] = Resource(\n",
    "                id=resource_id, available_windows=periods_list)\n",
    "\n",
    "    def create_resource_groups(self, resource_group_list):\n",
    "        # Generate Resource Groups\n",
    "        for x in resource_group_list:\n",
    "            resource_list = []\n",
    "            resources = x['resource_id']\n",
    "            for r in resources:\n",
    "                if r in self.dict_resource:\n",
    "                    resource_list.append(self.dict_resource[r])\n",
    "            \n",
    "            if resource_list:\n",
    "                self.dict_resourcegroups[x['resource_group_id']] = ResourceGroup(\n",
    "                    id=x['resource_group_id'], resources=resource_list)\n",
    "\n",
    "    def create_batch(self, task: Task, batch_size: int):\n",
    "        batches = TaskSplitter(task, batch_size).split_into_batches()\n",
    "        counter = 1\n",
    "        for batch in batches:\n",
    "            batch.id = f\"{task.id}-{counter}\"\n",
    "            counter += 1\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def create_task_object(self, task_list):\n",
    "        for i in task_list:\n",
    "            rg_list = []\n",
    "            task_id = i['taskno']\n",
    "            duration = int(i['duration'])\n",
    "            priority = int(i['priority'])\n",
    "            quantity = int(i['quantity'])\n",
    "            # micro_batch_size = int(\n",
    "            #     i['micro_batch_size']) if i['micro_batch_size'] else None\n",
    "            micro_batch_size = None\n",
    "            resource_group_id = i['resource_group_id']\n",
    "            rg_list = [self.dict_resourcegroups[g] for g in resource_group_id if g in self.dict_resourcegroups]\n",
    "            predecessors = i['predecessors']\n",
    "            parent_collection = i['parent_item_collection_id'] if micro_batch_size else None\n",
    "            predecessor_collection = i['predecessor_item_collection_id'] if micro_batch_size else None\n",
    "\n",
    "            assignments = []\n",
    "            # Create assignments \n",
    "            for x in resource_group_id: \n",
    "                if x in self.dict_resourcegroups:\n",
    "                    assignments.append(Assignment(resource_groups= [self.dict_resourcegroups[x]], resource_count= 1))\n",
    "                \n",
    "            # Temporarily add into component dicts\n",
    "            temp_task = Task(id=task_id,\n",
    "                             duration=duration,\n",
    "                             priority=priority,\n",
    "                             assignments= assignments,\n",
    "                             quantity=quantity)\n",
    "            \n",
    "            # Check for micro-batches\n",
    "            if not micro_batch_size:\n",
    "                self.task_dict[task_id] = temp_task  # Add task to dictionary\n",
    "\n",
    "                # Add predecessor to dictionary\n",
    "                self.pred_dict[task_id] = predecessors\n",
    "            else:\n",
    "                self.pred_dict[task_id] = predecessors\n",
    "                batches = self.create_batch(temp_task, micro_batch_size)\n",
    "                self.task_dict.update({task.id: task for task in batches})\n",
    "\n",
    "                # Temporarily copy the original predecessors for the new batches\n",
    "                self.pred_dict.update(\n",
    "                    {task.id: predecessors for task in batches})\n",
    "                self.flow_map.update({task.id: {\n",
    "                    \"parent\": parent_collection,\n",
    "                    \"predecessor\": predecessor_collection} for task in batches})\n",
    "                self.pred_exploded[task_id] = [task.id for task in batches]\n",
    "\n",
    "\n",
    "        # Organize predecessors for batches\n",
    "        for task in self.task_dict.values():\n",
    "            self.organize_predecessors(task)\n",
    "\n",
    "        # Add predecessors\n",
    "        for task in self.task_dict.values():\n",
    "            self.task_dict[task.id].predecessor_ids = [x for x in self.pred_dict[task.id] if x in self.task_dict] # Predecessor needs to be existing in task dictionary\n",
    "\n",
    "        # Build final task list\n",
    "        self.tasks_list = [value for key,\n",
    "                           value in sorted(self.task_dict.items())]\n",
    "        \n",
    "        # Convert periods to time\n",
    "    def int_to_datetime(self, num, start_time):\n",
    "        try:\n",
    "            # Parse the start time string into a datetime object\n",
    "            start_datetime = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            # Add the number of minutes to the start datetime\n",
    "            delta = timedelta(minutes=num)\n",
    "            result_datetime = start_datetime + delta\n",
    "            return result_datetime\n",
    "        \n",
    "        except Exception as e: \n",
    "            print(num)\n",
    "    \n",
    "    def run_scheduler(self):\n",
    "        self.create_resource_object(self.r_data)\n",
    "        print(\"Resource Objects Created.\")\n",
    "\n",
    "        self.create_resource_groups(self.rg_data)\n",
    "        print(\"Resource Groups Created.\")\n",
    "\n",
    "        self.create_task_object(self.t_data)\n",
    "        print(\"Task Objects Created.\")\n",
    "        print(f\"Original Task Length: {len(self.t_data)} | Post-Batched Length: {len(self.task_dict.values())}\")\n",
    "\n",
    "\n",
    "        self.solution = Scheduler(self.tasks_list, list(self.dict_resource.values())).schedule()\n",
    "        print(\"Solution Created.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ProdScheduler(\n",
    "    resource_dir = 'inputs/actual_data/resource.json',\n",
    "    resource_group_dir = 'inputs/actual_data/resourcegroups.json',\n",
    "    task_dir = 'inputs/actual_data/tasks_all.json'\n",
    ")\n",
    "\n",
    "scheduler.run_scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.dict_resource[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.task_dict['WO138769-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['start_dt'] = result.apply(lambda x: scheduler.int_to_datetime(x['task_start'], scheduler.today_str), axis=1)\n",
    "result['end_dt'] = result.apply(lambda x: scheduler.int_to_datetime(x['task_end'], scheduler.today_str), axis=1)\n",
    "result.sort_values(by='task_end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['task_id'].str.startswith('WO137709')].sort_values(by='task_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.task_dict['WO135483-40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = scheduler.solution.get_resource_intervals_df()\n",
    "intervals['start_dt'] = intervals.apply(lambda x: scheduler.int_to_datetime(x['interval_start'], scheduler.today_str), axis=1)\n",
    "intervals['end_dt'] = intervals.apply(lambda x: scheduler.int_to_datetime(x['interval_end'], scheduler.today_str), axis=1)\n",
    "intervals.sort_values(by='interval_end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
